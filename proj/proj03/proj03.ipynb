{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from stargazer.stargazer import Stargazer\n",
    "from random import choices  # To randomly choose clusters\n",
    "from sklearn.utils import resample \n",
    "from statsmodels.stats.multitest import multipletests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "baseline = pd.read_stata('data/baseline.dta')\n",
    "bok_inflation = pd.read_stata('data/BOK_inflation.dta')\n",
    "cleanpricedata_y1y2 = pd.read_stata('data/cleanPriceData_Y1Y2.dta')\n",
    "intensity_obs_short = pd.read_stata('data/intensity_obs_short.dta')\n",
    "lrfu_select_dataset = pd.read_stata('data/LRFU_select_dataset.dta')\n",
    "ms1ms2_pooled = pd.read_stata('data/MS1MS2_pooled.dta')\n",
    "repayment_datay1 = pd.read_stata('data/repayment_dataY1.dta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by cleaning the ms1ms2_pooled and baseline data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1019\n"
     ]
    }
   ],
   "source": [
    "# clean ms1ms2_pooled (drop if MS !=2, keep columns oafid and treatMS1MS2, group by oafid and take mean and rename)\n",
    "ms1ms2_pooled_tab1 = ms1ms2_pooled[ms1ms2_pooled['MS']==2]\n",
    "ms1ms2_pooled_tab1 = ms1ms2_pooled_tab1[['oafid', 'treatMS1MS2']]\n",
    "ms1ms2_pooled_tab1 = ms1ms2_pooled_tab1.groupby('oafid', as_index=False).mean()\n",
    "ms1ms2_pooled_tab1.rename(columns={'treatMS1MS2': 'treat13'}, inplace=True)\n",
    "print(ms1ms2_pooled_tab1.shape[0]) # checking we have the right number of observations as described in the original article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseline data we note that some of the columns have already been renamed with the suffix `_base` however and thus need to account for this. We however, assume that the data have not been altered in any other way compared to what the do in the `do` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yw/jsw5n53s1cb1s2q6tt0msrm00000gn/T/ipykernel_43037/2284489521.py:16: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'nan' has dtype incompatible with bool, please explicitly cast to a compatible dtype first.\n",
      "  baseline_clean.loc[baseline_clean['treatment2012'] == '', 'treat12'] = np.nan\n"
     ]
    }
   ],
   "source": [
    "# clean baseline data (the stata code indicates that the variables columns 'businessprofitmonth' and 'delta' should be kept, however they have already been renamed to 'businessprofitmonth_base' and 'delta_base')\n",
    "base_cols = ['oafid', 'logtotcons_base', 'male', 'num_adults', 'num_schoolchildren', 'finished_primary',\n",
    "                   'finished_secondary', 'cropland', 'num_rooms', 'schoolfees', 'totcons_base', 'logpercapcons_base',\n",
    "                   'total_cash_savings_base', 'total_cash_savings_trimmed', 'has_savings_acct', 'taken_bank_loan',\n",
    "                   'taken_informal_loan', 'liquidWealth', 'wagepay', 'businessprofitmonth_base', 'price_avg_diff_pct',\n",
    "                   'price_expect_diff_pct', 'harvest2011', 'netrevenue2011', 'netseller2011', 'autarkic2011',\n",
    "                   'maizelostpct2011', 'harvest2012', 'correct_interest', 'digit_recall', 'maizegiver', 'delta_base', 'treatment']\n",
    "baseline_clean = baseline[base_cols].copy()\n",
    "\n",
    "# rename columns\n",
    "baseline_clean.columns = [col + '_base' if not col.endswith('_base') and col != 'oafid' and col != 'treatment' else col for col in baseline_clean.columns]\n",
    "baseline_clean.rename(columns={'treatment': 'treatment2012'}, inplace=True)\n",
    "\n",
    "# generate treat12 as bool for treatment and control in 2012\n",
    "baseline_clean['treat12'] = baseline_clean['treatment2012'].apply(lambda x: x in ['T1', 'T2'])\n",
    "baseline_clean.loc[baseline_clean['treatment2012'] == '', 'treat12'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can merge the two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge baseline_clean and ms1ms2_pooled_clean on oafid\n",
    "base_ms1ms2_pool = pd.merge(baseline_clean, ms1ms2_pooled_tab1, on='oafid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "Baseline characteristic & Treat & Control & Obs & Std diff & P-val \\\\\n",
      "\\midrule\n",
      "Male & 0.296 & 0.334 & 1589 & -0.083 & 0.109 \\\\\n",
      "Number of adults & 3.004 & 3.196 & 1510 & -0.099 & 0.067 \\\\\n",
      "Children in school & 2.998 & 3.072 & 1589 & -0.038 & 0.454 \\\\\n",
      "Finished primary school & 0.718 & 0.772 & 1490 & -0.122 & 0.019 \\\\\n",
      "Finished secondary school & 0.253 & 0.270 & 1490 & -0.039 & 0.460 \\\\\n",
      "Total cropland (acres) & 2.441 & 2.398 & 1512 & 0.014 & 0.796 \\\\\n",
      "Number of rooms in household & 3.073 & 3.252 & 1511 & -0.072 & 0.219 \\\\\n",
      "Total school fees & 27239.693 & 29813.631 & 1589 & -0.068 & 0.191 \\\\\n",
      "Average monthly consumption (Ksh) & 14970.862 & 15371.378 & 1437 & -0.032 & 0.550 \\\\\n",
      "Average monthly consumption/capita (log) & 7.975 & 7.963 & 1434 & 0.019 & 0.721 \\\\\n",
      "Total cash savings (Ksh) & 5157.396 & 8021.499 & 1572 & -0.128 & 0.028 \\\\\n",
      "Total cash savings (trim) & 4731.623 & 5389.836 & 1572 & -0.050 & 0.343 \\\\\n",
      "Has bank savings acct & 0.419 & 0.425 & 1589 & -0.012 & 0.815 \\\\\n",
      "Taken bank loan & 0.079 & 0.083 & 1589 & -0.018 & 0.730 \\\\\n",
      "Taken informal loan & 0.244 & 0.249 & 1589 & -0.011 & 0.836 \\\\\n",
      "Liquid wealth (Ksh) & 93878.938 & 97280.922 & 1491 & -0.032 & 0.547 \\\\\n",
      "Off-farm wages (Ksh) & 3916.817 & 3797.480 & 1589 & 0.010 & 0.854 \\\\\n",
      "Business profit (Ksh) & 2302.588 & 1801.685 & 1589 & 0.051 & 0.265 \\\\\n",
      "Avg $\\%\\Delta$ price Sep-Jun & 133.495 & 133.178 & 1504 & 0.004 & 0.939 \\\\\n",
      "Expect $\\%\\Delta$ price Sep12-Jun13 & 124.680 & 117.255 & 1510 & 0.075 & 0.103 \\\\\n",
      "2011 LR harvest (bags) & 9.364 & 9.025 & 1511 & 0.022 & 0.670 \\\\\n",
      "Net revenue 2011 (Ksh) & -3303.691 & -4088.622 & 1428 & 0.017 & 0.716 \\\\\n",
      "Net seller 2011 & 0.324 & 0.303 & 1428 & 0.046 & 0.393 \\\\\n",
      "Autarkic 2011 & 0.068 & 0.060 & 1589 & 0.034 & 0.506 \\\\\n",
      "\\% maize lost 2011 & 0.016 & 0.013 & 1428 & 0.030 & 0.563 \\\\\n",
      "2012 LR harvest (bags) & 11.181 & 11.030 & 1484 & 0.018 & 0.733 \\\\\n",
      "Calculated interest correctly & 0.715 & 0.730 & 1580 & -0.034 & 0.502 \\\\\n",
      "Digit span recall & 4.568 & 4.576 & 1504 & -0.007 & 0.890 \\\\\n",
      "Maize giver & 0.261 & 0.261 & 1589 & -0.001 & 0.985 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create table 1\n",
    "# copy in case we need this later\n",
    "df_tab1 = base_ms1ms2_pool.copy()\n",
    "df_tab1['schoolfees_base'] = df_tab1['schoolfees_base']*1000\n",
    "\n",
    "# var list for table 1\n",
    "vars_list = [\n",
    "    \"male_base\", \"num_adults_base\", \"num_schoolchildren_base\", \"finished_primary_base\",\n",
    "    \"finished_secondary_base\", \"cropland_base\", \"num_rooms_base\", \"schoolfees_base\",\n",
    "    \"totcons_base\", \"logpercapcons_base\", \"total_cash_savings_base\",\n",
    "    \"total_cash_savings_trimmed_base\", \"has_savings_acct_base\", \"taken_bank_loan_base\",\n",
    "    \"taken_informal_loan_base\", \"liquidWealth_base\", \"wagepay_base\",\n",
    "    \"businessprofitmonth_base\", \"price_avg_diff_pct_base\",\n",
    "    \"price_expect_diff_pct_base\", \"harvest2011_base\", \"netrevenue2011_base\",\n",
    "    \"netseller2011_base\", \"autarkic2011_base\", \"maizelostpct2011_base\",\n",
    "    \"harvest2012_base\", \"correct_interest_base\", \"digit_recall_base\",\n",
    "    \"maizegiver_base\"\n",
    "]\n",
    "\n",
    "renaming = {\n",
    "    \"male_base\": \"Male\",\n",
    "    \"num_adults_base\": \"Number of adults\",\n",
    "    \"num_schoolchildren_base\": \"Children in school\",\n",
    "    \"finished_primary_base\": \"Finished primary school\",\n",
    "    \"finished_secondary_base\": \"Finished secondary school\",\n",
    "    \"cropland_base\": \"Total cropland (acres)\",\n",
    "    \"num_rooms_base\": \"Number of rooms in household\",\n",
    "    \"schoolfees_base\": \"Total school fees\",\n",
    "    \"totcons_base\": \"Average monthly consumption (Ksh)\",\n",
    "    \"logpercapcons_base\": \"Average monthly consumption/capita (log)\",\n",
    "    \"total_cash_savings_base\": \"Total cash savings (Ksh)\",\n",
    "    \"total_cash_savings_trimmed_base\": \"Total cash savings (trim)\",\n",
    "    \"has_savings_acct_base\": \"Has bank savings acct\",\n",
    "    \"taken_bank_loan_base\": \"Taken bank loan\",\n",
    "    \"taken_informal_loan_base\": \"Taken informal loan\",\n",
    "    \"liquidWealth_base\": \"Liquid wealth (Ksh)\",\n",
    "    \"wagepay_base\": \"Off-farm wages (Ksh)\",\n",
    "    \"businessprofitmonth_base\": \"Business profit (Ksh)\",\n",
    "    \"price_avg_diff_pct_base\": \"Avg $\\%\\Delta$ price Sep-Jun\",\n",
    "    \"price_expect_diff_pct_base\": \"Expect $\\%\\Delta$ price Sep12-Jun13\",\n",
    "    \"harvest2011_base\": \"2011 LR harvest (bags)\",\n",
    "    \"netrevenue2011_base\": \"Net revenue 2011 (Ksh)\",\n",
    "    \"netseller2011_base\": \"Net seller 2011\",\n",
    "    \"autarkic2011_base\": \"Autarkic 2011\",\n",
    "    \"maizelostpct2011_base\": \"\\% maize lost 2011\",\n",
    "    \"harvest2012_base\": \"2012 LR harvest (bags)\",\n",
    "    \"correct_interest_base\": \"Calculated interest correctly\",\n",
    "    \"digit_recall_base\": \"Digit span recall\",\n",
    "    \"maizegiver_base\": \"Maize giver\"\n",
    "}\n",
    "\n",
    "# function to perform t-tests\n",
    "def t_test_by_group(df, var, group_var='treat12'):\n",
    "    group1 = df[df[group_var] == 0][var].dropna()\n",
    "    group2 = df[df[group_var] == 1][var].dropna()\n",
    "    t_stat, p_val = stats.ttest_ind(group1, group2, equal_var=False)\n",
    "    return group1.mean(), group2.mean(), len(group1) + len(group2), t_stat, p_val\n",
    "\n",
    "# applying t-tests and collecting results\n",
    "results = []\n",
    "for var in vars_list:\n",
    "    control_mean, treat_mean, obs, t_stat, p_val = t_test_by_group(df_tab1, var)\n",
    "    std_diff = (treat_mean - control_mean) / np.sqrt(((len(df_tab1[df_tab1['treat12'] == 0][var]) - 1) * np.std(df_tab1[df_tab1['treat12'] == 0][var], ddof=1) ** 2 + (len(df_tab1[df_tab1['treat12'] == 1][var]) - 1) * np.std(df_tab1[df_tab1['treat12'] == 1][var], ddof=1) ** 2) / (len(df_tab1[df_tab1['treat12'] == 0][var]) + len(df_tab1[df_tab1['treat12'] == 1][var]) - 2))\n",
    "    results.append([var, treat_mean, control_mean, obs, std_diff, p_val])\n",
    "\n",
    "# convert results to a df to use pandas output to latex\n",
    "results_df = pd.DataFrame(results, columns=['Variable', 'Treat Mean', 'Control Mean', 'Observations', 'Std Diff', 'P-value'])\n",
    "results_df['Variable'] = results_df['Variable'].map(renaming)\n",
    "results_df = results_df.rename(columns={\n",
    "    'Variable':'Baseline characteristic', \n",
    "    'Treat Mean':'Treat', \n",
    "    'Control Mean':'Control', \n",
    "    'Observations':'Obs', \n",
    "    'Std Diff':'Std diff', \n",
    "    'P-value':'P-val'})\n",
    "\n",
    "latex_table1 = results_df.to_latex(index=False, float_format=\"%.3f\")\n",
    "print(latex_table1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Table 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms1ms2_pooled_tab5 = ms1ms2_pooled.copy(deep=True)\n",
    "max_strata_group = ms1ms2_pooled_tab5['strata_group'].max()\n",
    "ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5['MS'] == 2, 'strata_group'] = ms1ms2_pooled_tab5['groupstrata'] + max_strata_group\n",
    "\n",
    "ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5['MS'] == 2, 'oafid'] = ms1ms2_pooled_tab5['fr_id']\n",
    "\n",
    "ms1ms2_pooled_tab5['purchasequant2'] = ms1ms2_pooled_tab5['purchasequant']\n",
    "ms1ms2_pooled_tab5.loc[(ms1ms2_pooled_tab5['purchaseval']==0)&(ms1ms2_pooled_tab5['purchasequant'].isna()),'purchasequant2'] = 0\n",
    "ms1ms2_pooled_tab5['netsales2'] = ms1ms2_pooled_tab5['salesquant'] - ms1ms2_pooled_tab5['purchasequant2']\n",
    "ms1ms2_pooled_tab5['netsales'] = ms1ms2_pooled_tab5['netsales2']\n",
    "\n",
    "ms1ms2_pooled_tab5.drop(columns=['netsales_trim','purchaseval_trim','salesval_trim'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim outliers\n",
    "for x in ['purchaseval', 'salesval', 'purchasequant', 'salesquant']:\n",
    "    quantile = ms1ms2_pooled_tab5[x].quantile([0.99])\n",
    "    ms1ms2_pooled_tab5[f'{x}_trim'] = ms1ms2_pooled_tab5[x]\n",
    "    ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5[f'{x}_trim'] > quantile[0.99],f'{x}_trim'] = np.nan\n",
    "\n",
    "quantile = ms1ms2_pooled_tab5['netsales'].quantile([0.005, 0.995])\n",
    "ms1ms2_pooled_tab5['netsales_trim'] = ms1ms2_pooled_tab5['netsales']\n",
    "ms1ms2_pooled_tab5.loc[(ms1ms2_pooled_tab5['netsales_trim'] <= quantile[0.005]) | (ms1ms2_pooled_tab5['netsales_trim'] > quantile[0.995]) , 'netsales_trim'] = np.nan\n",
    "\n",
    "# create id\n",
    "ms1ms2_pooled_tab5['id'] = ms1ms2_pooled_tab5['oafid'].fillna(ms1ms2_pooled_tab5['fr_id'])\n",
    "\n",
    "# create effective prices\n",
    "trim_vars = ['salesquant_trim', 'purchasequant_trim', 'salesval_trim', 'purchaseval_trim']\n",
    "for var in trim_vars:\n",
    "    ms1ms2_pooled_tab5[f'tot_{var}'] = ms1ms2_pooled_tab5.groupby(['id', 'MS'])[var].transform('sum')\n",
    "\n",
    "for x in ['purchase', 'sales']:\n",
    "    ms1ms2_pooled_tab5[f'effective_{x}_price'] = ms1ms2_pooled_tab5[f'tot_{x}val_trim'] / ms1ms2_pooled_tab5[f'tot_{x}quant_trim']\n",
    "    ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5[f'tot_{x}quant_trim']== 0,f'effective_{x}_price'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for net sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# define variable\n",
    "dv = 'netsales_trim'\n",
    "independent_vars = ['z', 'treatMS1MS2_1 + treatMS1MS2_2 + treatMS1MS2_3']\n",
    "\n",
    "for i, var in enumerate(independent_vars):\n",
    "    df = ms1ms2_pooled_tab5.copy(deep=True)\n",
    "    df['z'] = df['treatMS1MS2']\n",
    "    if var == 'z':\n",
    "        df.dropna(subset=[dv,'z','interviewdate','Y1round2','Y1round3','Y2round1','Y2round2','Y2round3','strata_group','groupnum'], inplace=True)\n",
    "    else:\n",
    "        df.dropna(subset=[dv,'treatMS1MS2_1','treatMS1MS2_2','treatMS1MS2_3','interviewdate','Y1round2','Y1round3','Y2round1','Y2round2','Y2round3','strata_group','groupnum'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    formula = f'{dv} ~ {var} + interviewdate + Y1round2 + Y1round3 + Y2round1 + Y2round2 + Y2round3 + C(strata_group)'\n",
    "    model = smf.ols(formula, df).fit(cov_type='cluster', cov_kwds={'groups': df['groupnum']})\n",
    "    mean_dev = df.loc[df['treatMS1MS2'] == 0, dv].mean()\n",
    "    std_dev = df.loc[df['treatMS1MS2'] == 0, dv].std()\n",
    "    fwer_pvals = multipletests(model.pvalues, method='fdr_bh')[1]\n",
    "    results[f'netsales_{i}'] = {'model':model, 'mean_dev':mean_dev, 'std_dev':std_dev, 'fwer_pvals':fwer_pvals}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for effective price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dv in ['purchase', 'sales']:\n",
    "    for i, treat in enumerate(['treat12', 'treat13', 'treatMS1MS2']):\n",
    "        df = ms1ms2_pooled_tab5.copy(deep=True)\n",
    "        df['z'] = df[treat]\n",
    "        df = df.drop_duplicates(subset=['id', 'MS'], keep='first')\n",
    "        df.dropna(subset=[f'effective_{dv}_price','z','groupnum'], inplace=True)\n",
    "        if treat == 'treatMS1MS2':\n",
    "            formula = f'effective_{dv}_price ~ z + C(strata_group)'\n",
    "        else:\n",
    "            df = df[df['MS'] == i+1]\n",
    "            formula = f'effective_{dv}_price ~ z + C(strata_group)'\n",
    "        model = smf.ols(formula, data=df).fit(cov_type='cluster', cov_kwds={'groups': df['groupnum']})\n",
    "        mean_dev = df.loc[df['z'] == 0, f'effective_{dv}_price'].mean()\n",
    "        std_dev = df.loc[df['z'] == 0, f'effective_{dv}_price'].std()\n",
    "        fwer_pvals = multipletests(model.pvalues['z'], method='fdr_bh')[1]\n",
    "        results[f'{dv}_{treat}'] = {'model':model, 'mean_dev':mean_dev, 'std_dev':std_dev, 'fwer_pvals':fwer_pvals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{@{\\extracolsep{5pt}}lcccc}\n",
      "\\\\[-1.8ex]\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "\\\\[-1.8ex] & \\multicolumn{2}{c}{Net Sales} & \\multicolumn{2}{c}{Effective Price}  \\\\\n",
      "\\\\[-1.8ex] & (1) & (2) & (3) & (4) \n",
      " \\\\ & Overall & By rd & Purchase & Sales \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Treat & 0.193$^{***}$ & & -57.449$^{**}$ & 145.509$^{***}$ \\\\\n",
      "& (0.064) & & (27.156) & (41.767) \\\\\n",
      " Treat - R1 & & -0.173$^{*}$ & & \\\\\n",
      "& & (0.095) & & \\\\\n",
      " Treat - R2 & & 0.376$^{***}$ & & \\\\\n",
      "& & (0.102) & & \\\\\n",
      " Treat - R3 & & 0.366$^{***}$ & & \\\\\n",
      "& & (0.091) & & \\\\\n",
      " P-value Treat &  &  & 0.034 & 0.0 \\\\\n",
      " P-value Treat FWER &  &  & 0.034 & 0.0 \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Observations & 6736 & 6736 & 2014 & 1428 \\\\\n",
      " $R^2$ & 0.100 & 0.104 & 0.089 & 0.066 \\\\\n",
      " % Adjusted $R^2$ & 0.091 & 0.094 & 0.060 & 0.024 \\\\\n",
      " % Residual Std. Error & 1.998 & 1.994 & 639.432 & 789.099 \\\\\n",
      " % F Statistic & 33.380$^{***}$ & 2.850$^{***}$ & 34.249$^{***}$ & 13.002$^{***}$ \\\\\n",
      "\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "% \\textit{Note:} & \\multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 68, but rank is 66\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 70, but rank is 68\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 62, but rank is 61\n",
      "  warnings.warn('covariance of constraints does not have full '\n",
      "/opt/anaconda3/lib/python3.11/site-packages/statsmodels/base/model.py:1888: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 62, but rank is 61\n",
      "  warnings.warn('covariance of constraints does not have full '\n"
     ]
    }
   ],
   "source": [
    "models_list = ['netsales_0','netsales_1','purchase_treatMS1MS2','sales_treatMS1MS2']\n",
    "stargazer = Stargazer([results[model]['model'] for model in models_list])\n",
    "\n",
    "# get p-values\n",
    "pvals = np.round(pd.DataFrame({model:results[model]['model'].pvalues for model in ['purchase_treatMS1MS2','sales_treatMS1MS2']}),3)\n",
    "fwer_pvals = np.round(pd.DataFrame({model:results[model]['fwer_pvals'] for model in ['purchase_treatMS1MS2','sales_treatMS1MS2']}),3)\n",
    "# rename columns\n",
    "for i, df in enumerate([pvals, fwer_pvals]):\n",
    "    df.columns = ['Purchase', 'Sales']\n",
    "    df['Overall'] = \"\"\n",
    "    df['By rd'] = \"\"\n",
    "    # reorder columns\n",
    "    if i == 0:\n",
    "        pvals = df[['Overall', 'By rd', 'Purchase', 'Sales']]\n",
    "    else:\n",
    "        fwer_pvals = df[['Overall', 'By rd', 'Purchase', 'Sales']]\n",
    "\n",
    "# configure Stargazer object for output\n",
    "stargazer.custom_columns(['Net Sales', 'Effective Price'], [2, 2])\n",
    "stargazer.rename_covariates({'z': 'Treat','treatMS1MS2_1':'Treat - R1', 'treatMS1MS2_2':'Treat - R2', 'treatMS1MS2_3':'Treat - R3'})\n",
    "stargazer.show_degrees_of_freedom(False)\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.covariate_order(['z', 'treatMS1MS2_1', 'treatMS1MS2_2', 'treatMS1MS2_3'])\n",
    "# add p-values as a rows \n",
    "stargazer.add_line('P-value Treat', pvals.loc['z'].values.tolist())\n",
    "stargazer.add_line('P-value Treat FWER', fwer_pvals.loc[0].values.tolist())\n",
    "\n",
    "latex_table5 = stargazer.render_latex()\n",
    "latex_table5 = latex_table5.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) \\\\\",\n",
    "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) \\n \\\\\\ & Overall & By rd & Purchase & Sales \\\\\")\n",
    "latex_table5 = latex_table5.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
    "latex_table5 = latex_table5.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
    "latex_table5 = latex_table5.replace(\"F Statistic\", \"% F Statistic\")\n",
    "latex_table5 = latex_table5.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
    "latex_table5 = latex_table5.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
    "latex_table5 = latex_table5.replace(\"\\\\end{table}\", \"\")\n",
    "\n",
    "print(latex_table5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Table 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanpricedata_y1y2_tab6 = cleanpricedata_y1y2.copy(deep=True)\n",
    "cleanpricedata_y1y2_tab6 = cleanpricedata_y1y2_tab6[['salesPrice_trim','hi_1km_wt','hi_3km_wt','hi_5km_wt','monthnum','subloc_1km_wt_grp','subloc_3km_wt_grp','subloc_5km_wt_grp', 'in_sample','MS','lean']]\n",
    "cleanpricedata_y1y2_tab6['hi'] = pd.NA\n",
    "cleanpricedata_y1y2_tab6['interact'] = pd.NA\n",
    "cleanpricedata_y1y2_tab6['interact_lean'] = pd.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for dist in ['1km_wt', '3km_wt', '5km_wt']:\n",
    "    df = cleanpricedata_y1y2_tab6.copy(deep=True)\n",
    "    df.dropna(subset=[f'hi_{dist}','salesPrice_trim','monthnum'], inplace=True)\n",
    "    mean_price = df[(df['monthnum'] == 0) & (df[f'hi_{dist}'] == 0)]['salesPrice_trim'].mean()\n",
    "    norm = 100 / mean_price\n",
    "\n",
    "    # normalize price\n",
    "    df['salesPrice_trim_norm'] = df['salesPrice_trim'] * norm\n",
    "\n",
    "    # create hi variable\n",
    "    df['hi'] = df[f'hi_{dist}']\n",
    "    df['interact'] = df['monthnum'] * df['hi']\n",
    "\n",
    "    # regression\n",
    "    formula = 'salesPrice_trim_norm ~ hi + monthnum + interact'\n",
    "\n",
    "    for ms in [1,2,3]: # 3 is pooled\n",
    "        if ms == 3:\n",
    "            df_filt = df[(df['in_sample'] == 1)]\n",
    "        else:\n",
    "            df_filt = df[(df['MS'] == ms) & (df['in_sample'] == 1)]\n",
    "        model = smf.ols(formula=formula, data=df_filt).fit(cov_type='cluster', cov_kwds={'groups': df_filt[f'subloc_{dist}_grp']})\n",
    "        results[(dist, ms)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvals = pd.DataFrame()\n",
    "# storeing pval in a df\n",
    "for dv in ['hi', 'monthnum', 'interact']:\n",
    "    val = {(k[0], k[1]): np.round(v.pvalues[dv],3) for k, v in results.items()}\n",
    "    pvals[dv] = pd.Series(val)\n",
    "    \n",
    "# keep only columns 3km_wt and 3rd column in 1km_wt and 5km_wt\n",
    "pvals = pvals.T\n",
    "pvals = pvals[[('3km_wt', 1), ('3km_wt', 2), ('3km_wt', 3), ('1km_wt', 3), ('5km_wt', 3)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wild_bootstrap(data, model, n_bootstraps, dv, clust_var):\n",
    "    \"\"\"\n",
    "    Wild Cluster Bootstrap-t with random signs within clusters\n",
    "    \"\"\"\n",
    "    cluster_var = data[clust_var]\n",
    "    unique_clusters = cluster_var.unique()\n",
    "    boot_results = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        boot_data = data.copy()\n",
    "        # resample residuals within each cluster\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_indices = data[cluster_var == cluster].index\n",
    "\n",
    "            # multiply residuals by random signs, either -1 or 1, within each cluster\n",
    "            signs = np.random.choice([-1, 1], size=len(cluster_indices))\n",
    "            boot_data.loc[cluster_indices, dv] = model.predict(data.loc[cluster_indices]) + signs * model.resid.loc[cluster_indices]\n",
    "\n",
    "        # Refit model on bootstrapped data\n",
    "        boot_model = smf.ols(model.model.formula, data=boot_data).fit()\n",
    "        boot_results.append(boot_model.params)\n",
    "    \n",
    "    return np.array(boot_results)[:,1:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_res = {}\n",
    "bootstrap_pvals = pd.DataFrame(index=pd.MultiIndex.from_product([['1km_wt', '3km_wt', '5km_wt'], [1, 2, 3]], names=['dist', 'ms']), columns=['hi', 'monthnum', 'interact'])\n",
    "n_bootstraps = 999\n",
    "\n",
    "for dist  in ['1km_wt', '3km_wt', '5km_wt']:\n",
    "    df = cleanpricedata_y1y2_tab6.copy(deep=True)\n",
    "    df.dropna(subset=[f'hi_{dist}','salesPrice_trim','monthnum'], inplace=True)\n",
    "    mean_price = df[(df['monthnum'] == 0) & (df[f'hi_{dist}'] == 0)]['salesPrice_trim'].mean()\n",
    "    norm = 100 / mean_price\n",
    "\n",
    "    # normalize price\n",
    "    df['salesPrice_trim_norm'] = df['salesPrice_trim'] * norm\n",
    "    df['salesPrice_trim_norm'] = df['salesPrice_trim_norm'].astype(float)\n",
    "\n",
    "    # create hi variable\n",
    "    df['hi'] = df[f'hi_{dist}']\n",
    "    df['interact'] = df['monthnum'] * df['hi']\n",
    "\n",
    "    # regression\n",
    "    formula = 'salesPrice_trim_norm ~ hi + monthnum + interact'\n",
    "\n",
    "    for ms in [1,2,3]: # 3 is pooled\n",
    "        if ms == 3:\n",
    "            df_filt = df[(df['in_sample'] == 1)]\n",
    "        else:\n",
    "            df_filt = df[(df['MS'] == ms) & (df['in_sample'] == 1)]\n",
    "        res = wild_bootstrap(df_filt, results[(dist, ms)], n_bootstraps, 'salesPrice_trim_norm', f'subloc_{dist}_grp')\n",
    "        bootstrap_res[(dist,ms)] = res\n",
    "\n",
    "        model = results[(dist, ms)]\n",
    "\n",
    "        for i, var in enumerate(['hi', 'monthnum', 'interact']):\n",
    "            observed_coef = model.params[var]\n",
    "            # calculating p-values as proportion of bootstrap coefs where abs(boot_coef) >= abs(obs_coef)\n",
    "            p_value = np.round(np.mean(np.abs(bootstrap_res[(dist,ms)][:,i]) >= np.abs(observed_coef)),3)\n",
    "            \n",
    "            # store p-value in df \n",
    "            bootstrap_pvals.loc[(dist,ms),var] = p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_pvals = bootstrap_pvals.T\n",
    "bootstrap_pvals = bootstrap_pvals[[('3km_wt', 1), ('3km_wt', 2), ('3km_wt', 3), ('1km_wt', 3), ('5km_wt', 3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{@{\\extracolsep{5pt}}lccccc}\n",
      "\\\\[-1.8ex]\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "& \\multicolumn{5}{c}{\\textit{Dependent variable: Normalized sales price}} \\\n",
      "\\cr \\cline{2-6}\n",
      "\\\\[-1.8ex] & \\multicolumn{3}{c}{Main Specification (3km)} & \\multicolumn{2}{c}{Robustness (Pooled)}  \\\\\n",
      "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \n",
      " \\\\ & Y1 & Y2 & Pooled & 1km & 5km \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " High & 4.410$^{**}$ & 2.855$^{}$ & 3.970$^{**}$ & 2.787$^{}$ & 3.766$^{**}$ \\\\\n",
      "& (2.091) & (1.992) & (1.817) & (1.719) & (1.822) \\\\\n",
      " Month & 1.189$^{***}$ & 1.224$^{***}$ & 1.364$^{***}$ & 1.327$^{***}$ & 1.537$^{***}$ \\\\\n",
      "& (0.363) & (0.377) & (0.350) & (0.339) & (0.291) \\\\\n",
      " High x Month & -0.574$^{}$ & -0.476$^{}$ & -0.573$^{}$ & -0.520$^{}$ & -0.835$^{**}$ \\\\\n",
      "& (0.422) & (0.459) & (0.386) & (0.390) & (0.366) \\\\\n",
      " P-value High & 0.035 & 0.152 & 0.029 & 0.105 & 0.039 \\\\\n",
      " P-value Treat Bootstrap & 0.501 & 0.495 & 0.529 & 0.46 & 0.505 \\\\\n",
      " P-value Month & 0.001 & 0.001 & 0.0 & 0.0 & 0.0 \\\\\n",
      " P-value High Bootstrap & 0.524 & 0.492 & 0.502 & 0.47 & 0.512 \\\\\n",
      " P-value High x Month & 0.173 & 0.3 & 0.138 & 0.182 & 0.023 \\\\\n",
      " P-value Treat x High Bootstrap & 0.492 & 0.584 & 0.496 & 0.479 & 0.495 \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Observations & 491 & 381 & 872 & 872 & 872 \\\\\n",
      " $R^2$ & 0.077 & 0.031 & 0.058 & 0.055 & 0.060 \\\\\n",
      " % Adjusted $R^2$ & 0.071 & 0.023 & 0.055 & 0.052 & 0.056 \\\\\n",
      " % Residual Std. Error & 10.071 & 14.651 & 12.700 & 12.726 & 12.685 \\\\\n",
      " % F Statistic & 6.401$^{***}$ & 7.496$^{***}$ & 13.411$^{***}$ & 10.971$^{***}$ & 16.730$^{***}$ \\\\\n",
      "\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "% \\textit{Note:} & \\multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use stargazer to create a table\n",
    "result_list = [results[('3km_wt', 1)], results[('3km_wt', 2)], results[('3km_wt', 3)], results[('1km_wt', 3)], results[('5km_wt', 3)]]\n",
    "stargazer = Stargazer(result_list)\n",
    "\n",
    "# configure Stargazer object for output\n",
    "stargazer.custom_columns(['Main Specification (3km)', 'Robustness (Pooled)'], [3, 2])\n",
    "stargazer.rename_covariates({'hi': 'High', 'monthnum': 'Month', 'interact': 'High x Month'})\n",
    "stargazer.show_degrees_of_freedom(False)\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.covariate_order(['hi', 'monthnum', 'interact'])\n",
    "# add p-values as a rows \n",
    "stargazer.add_line('P-value High', pvals.loc['hi'].values.tolist())\n",
    "stargazer.add_line('P-value Treat Bootstrap', bootstrap_pvals.loc['hi'].values.tolist())\n",
    "stargazer.add_line('P-value Month', pvals.loc['monthnum'].values.tolist())\n",
    "stargazer.add_line('P-value High Bootstrap', bootstrap_pvals.loc['monthnum'].values.tolist())\n",
    "stargazer.add_line('P-value High x Month', pvals.loc['interact'].values.tolist())\n",
    "stargazer.add_line('P-value Treat x High Bootstrap', bootstrap_pvals.loc['interact'].values.tolist())\n",
    "\n",
    "\n",
    "latex_table6 = stargazer.render_latex()\n",
    "\n",
    "# edit the latex table to add row for telling if Y1 Y2 or Pooled after \\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\n",
    "latex_table6 = latex_table6.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \\\\\",\n",
    "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \\n \\\\\\ & Y1 & Y2 & Pooled & 1km & 5km \\\\\")\n",
    "latex_table6 = latex_table6.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
    "latex_table6 = latex_table6.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
    "latex_table6 = latex_table6.replace(\"F Statistic\", \"% F Statistic\")\n",
    "latex_table6 = latex_table6.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
    "latex_table6 = latex_table6.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
    "latex_table6 = latex_table6.replace(\"\\\\end{table}\", \"\")\n",
    "latex_table6 = latex_table6.replace('salesPrice_trim_norm', 'Normalized sales price')\n",
    "\n",
    "print(latex_table6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Table 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the raw data and create columns for treatment and interaction variable\n",
    "ms1ms2_pooled_tab7 = ms1ms2_pooled.copy(deep=True)\n",
    "# filter relevant columns\n",
    "ms1ms2_pooled_tab7 = ms1ms2_pooled_tab7[['oafid', # id\n",
    "                                         'treat12', 'treat13', 'treatMS1MS2', # treatment variables\n",
    "                                         'inventory_trim', 'netrevenue_trim', 'logtotcons_trim', # outcome variables\n",
    "                                         'Y1round2', 'Y1round3', 'Y2round1', 'Y2round2', 'Y2round3','hi','subloc','interviewdate']] # independent variables\n",
    "\n",
    "ms1ms2_pooled_tab7.sort_index(inplace=True)\n",
    "ms1ms2_pooled_tab7['z'] = pd.NA\n",
    "ms1ms2_pooled_tab7['z_hi'] = pd.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the first set of regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of treaments\n",
    "treatments = ['treat12', 'treat13', 'treatMS1MS2']\n",
    "\n",
    "# list of dependent variables\n",
    "dependent_vars = ['inventory_trim', 'netrevenue_trim', 'logtotcons_trim']\n",
    "\n",
    "# list of changeing independent variables depending on the treatment\n",
    "independent_vars = {\n",
    "    'treat12': 'Y1round2 + Y1round3',\n",
    "    'treat13': 'Y2round2 + Y2round3',\n",
    "    'treatMS1MS2': 'Y1round2 + Y1round3 + Y2round1 + Y2round2 + Y2round3'\n",
    "    }\n",
    "\n",
    "# empty dictionary to store results\n",
    "results = {}\n",
    "pvals = {var: [] for var in ['z', 'hi', 'z_hi','z+z_hi']}\n",
    "\n",
    "# Simulating the loop to replace variables and run regressions\n",
    "for dv in dependent_vars:\n",
    "    for treat in treatments:\n",
    "        # Stata automatically omits the missing values in the regression â€“ here we have to do it manually so we copy the data and drop variables\n",
    "        df = ms1ms2_pooled_tab7.copy(deep=True)\n",
    "        df = df.dropna(subset=[dv, treat, 'hi', 'subloc','interviewdate'])\n",
    "        # setting treament variable\n",
    "        df['z'] = df[treat] # setting z to the treatment variable\n",
    "        \n",
    "        # setting interaction variable\n",
    "        df['z_hi'] = df[treat]*df['hi'] # setting z_hi to the interaction of the treatment hi saturation\n",
    "        \n",
    "        # setting the formula to run the regression\n",
    "        formula = f'{dv} ~ z + hi + z_hi + interviewdate + {independent_vars[treat]}'\n",
    "\n",
    "        # Run the regression\n",
    "        model_key = f'model_{dependent_vars.index(dv)*len(treatments) + treatments.index(treat)}'\n",
    "        results[model_key] = smf.ols(formula, data=df).fit(cov_type='cluster', cov_kwds={'groups': df['subloc']})\n",
    "        # print(results[f'model_{i}'].summary())\n",
    "\n",
    "        # test the hypothesis that z + z_hi = 0\n",
    "        hypothesis = 'z + z_hi = 0'\n",
    "        t_test = results[model_key].t_test(hypothesis)\n",
    "\n",
    "        # store p-value round to 3 decimals\n",
    "        pvals['z+z_hi'].append(np.round(t_test.pvalue,3))\n",
    "        pvals['z'].append(np.round(results[model_key].pvalues['z'],3))\n",
    "        pvals['hi'].append(np.round(results[model_key].pvalues['hi'],3))\n",
    "        pvals['z_hi'].append(np.round(results[model_key].pvalues['z_hi'],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running bootstrap regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wild_bootstrap(data, model, n_bootstraps, dv, clust_var):\n",
    "    \"\"\"\n",
    "    Wild Cluster Bootstrap-t with random signs within clusters\n",
    "    \"\"\"\n",
    "    cluster_var = data[clust_var]\n",
    "    unique_clusters = cluster_var.unique()\n",
    "    boot_results = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        boot_data = data.copy()\n",
    "        # resample residuals within each cluster\n",
    "        for cluster in unique_clusters:\n",
    "            cluster_indices = data[cluster_var == cluster].index\n",
    "\n",
    "            # multiply residuals by random signs, either -1 or 1, within each cluster\n",
    "            signs = np.random.choice([-1, 1], size=len(cluster_indices))\n",
    "            boot_data.loc[cluster_indices, dv] = model.predict(data.loc[cluster_indices]) + signs * model.resid.loc[cluster_indices]\n",
    "\n",
    "        # Refit model on bootstrapped data\n",
    "        boot_model = smf.ols(model.model.formula, data=boot_data).fit()\n",
    "        boot_results.append(boot_model.params)\n",
    "    \n",
    "    return np.array(boot_results)[:,1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstraps = 10  # reported data is based on 1000 iterations\n",
    "bootstrap_res = {}\n",
    "bootstrap_pvals = {var: [] for var in ['z', 'hi', 'z_hi']}\n",
    "\n",
    "for dv in dependent_vars:\n",
    "    for treat in treatments:\n",
    "        df = ms1ms2_pooled_tab7.copy(deep=True)\n",
    "        df = df.dropna(subset=[dv, treat, 'hi', 'subloc', 'interviewdate'])\n",
    "        df['z'] = df[treat]\n",
    "        df['z_hi'] = df[treat] * df['hi']\n",
    "        df[dv] = df[dv].astype(float)\n",
    "        \n",
    "        formula = f'{dv} ~ z + hi + z_hi + interviewdate + {independent_vars[treat]}'\n",
    "        model_key = f'model_{dependent_vars.index(dv)*len(treatments) + treatments.index(treat)}'\n",
    "        model = results[model_key]\n",
    "\n",
    "        # Wild bootstrap\n",
    "        res = wild_bootstrap(df, model, n_bootstraps, dv, 'subloc')\n",
    "        bootstrap_res[model_key] = res\n",
    "\n",
    "        for i, var in enumerate(['z', 'hi', 'z_hi']):\n",
    "            observed_coef = model.params[var]\n",
    "            # calculating p-values as proportion of bootstrap coefs where abs(boot_coef) >= abs(obs_coef)\n",
    "            p_value = np.mean(np.abs(bootstrap_res[model_key][:,i]) >= np.abs(observed_coef))\n",
    "            bootstrap_pvals[var].append(p_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output code to LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\\begin{tabular}{@{\\extracolsep{5pt}}lccccccccc}\n",
      "\\\\[-1.8ex]\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "\\\\[-1.8ex] & \\multicolumn{3}{c}{Inventory} & \\multicolumn{3}{c}{Net Revenues} & \\multicolumn{3}{c}{Consumption}  \\\\\n",
      "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \n",
      " \\\\ & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Treat & 0.759$^{***}$ & 0.546$^{***}$ & 0.740$^{***}$ & 1059.602$^{**}$ & 1193.768$^{*}$ & 1101.389$^{**}$ & 0.012$^{}$ & -0.051$^{}$ & -0.011$^{}$ \\\\\n",
      "& (0.189) & (0.185) & (0.155) & (437.732) & (685.048) & (430.091) & (0.040) & (0.040) & (0.023) \\\\\n",
      " High & 0.124$^{}$ & -0.028$^{}$ & 0.017$^{}$ & 533.903$^{}$ & -152.603$^{}$ & 164.936$^{}$ & -0.003$^{}$ & -0.084$^{}$ & -0.047$^{}$ \\\\\n",
      "& (0.355) & (0.219) & (0.241) & (551.179) & (558.948) & (479.685) & (0.051) & (0.053) & (0.043) \\\\\n",
      " Treat x High & -0.333$^{}$ & -0.065$^{}$ & -0.291$^{}$ & -1114.628$^{**}$ & -555.215$^{}$ & -816.770$^{}$ & -0.013$^{}$ & 0.174$^{***}$ & 0.067$^{*}$ \\\\\n",
      "& (0.229) & (0.255) & (0.192) & (535.594) & (804.864) & (520.036) & (0.052) & (0.055) & (0.037) \\\\\n",
      " P-value T + TH = 0 & 0.002 & 0.006 & 0.002 & 0.861 & 0.125 & 0.396 & 0.969 & 0.001 & 0.063 \\\\\n",
      " P-value Treat & 0.0 & 0.003 & 0.0 & 0.015 & 0.081 & 0.01 & 0.764 & 0.209 & 0.621 \\\\\n",
      " P-value Treat Bootstrap & 0.7 & 0.9 & 0.4 & 0.6 & 0.6 & 0.6 & 0.8 & 0.5 & 0.6 \\\\\n",
      " P-value High & 0.726 & 0.899 & 0.944 & 0.333 & 0.785 & 0.731 & 0.961 & 0.115 & 0.279 \\\\\n",
      " P-value High Bootstrap & 0.8 & 0.9 & 1.0 & 0.3 & 0.8 & 0.6 & 1.0 & 0.5 & 0.6 \\\\\n",
      " P-value Treat x High & 0.146 & 0.799 & 0.13 & 0.037 & 0.49 & 0.116 & 0.798 & 0.002 & 0.072 \\\\\n",
      " P-value Treat x High Bootstrap & 0.9 & 0.9 & 0.5 & 0.5 & 0.5 & 0.5 & 0.9 & 0.6 & 0.4 \\\\\n",
      "\\hline \\\\[-1.8ex]\n",
      " Observations & 3836 & 2944 & 6780 & 3795 & 2935 & 6730 & 3792 & 2944 & 6736 \\\\\n",
      " $R^2$ & 0.346 & 0.184 & 0.293 & 0.009 & 0.043 & 0.091 & 0.002 & 0.017 & 0.025 \\\\\n",
      " % Adjusted $R^2$ & 0.345 & 0.182 & 0.292 & 0.008 & 0.041 & 0.090 & 0.000 & 0.015 & 0.024 \\\\\n",
      " % Residual Std. Error & 3.015 & 2.793 & 2.947 & 6188.647 & 6410.741 & 6286.767 & 0.621 & 0.647 & 0.633 \\\\\n",
      " % F Statistic & 369.556$^{***}$ & 93.029$^{***}$ & 364.779$^{***}$ & 2.004$^{*}$ & 19.627$^{***}$ & 119.335$^{***}$ & 0.616$^{}$ & 4.496$^{***}$ & 16.477$^{***}$ \\\\\n",
      "\\hline\n",
      "\\hline \\\\[-1.8ex]\n",
      "% \\textit{Note:} & \\multicolumn{9}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use stargazer to create a table\n",
    "result_list = list(results.values())\n",
    "stargazer = Stargazer(result_list)\n",
    "\n",
    "# configure Stargazer object for output\n",
    "stargazer.custom_columns(['Inventory', 'Net Revenues', 'Consumption'], [3, 3, 3])\n",
    "stargazer.rename_covariates({'z': 'Treat', 'hi': 'High', 'z_hi': 'Treat x High'})\n",
    "stargazer.show_degrees_of_freedom(False)\n",
    "stargazer.significant_digits(3)\n",
    "stargazer.covariate_order(['z', 'hi', 'z_hi'])\n",
    "# add p-values as a rows \n",
    "stargazer.add_line('P-value T + TH = 0', pvals['z+z_hi'])\n",
    "stargazer.add_line('P-value Treat', pvals['z'])\n",
    "stargazer.add_line('P-value Treat Bootstrap', bootstrap_pvals['z'])\n",
    "stargazer.add_line('P-value High', pvals['hi'])\n",
    "stargazer.add_line('P-value High Bootstrap', bootstrap_pvals['hi'])\n",
    "stargazer.add_line('P-value Treat x High', pvals['z_hi'])\n",
    "stargazer.add_line('P-value Treat x High Bootstrap', bootstrap_pvals['z_hi'])\n",
    "\n",
    "\n",
    "latex_table7 = stargazer.render_latex()\n",
    "\n",
    "# edit the latex table to add row for telling if Y1 Y2 or Pooled after \\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\n",
    "latex_table7 = latex_table7.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\",\n",
    "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\n \\\\\\ & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled \\\\\")\n",
    "latex_table7 = latex_table7.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
    "latex_table7 = latex_table7.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
    "latex_table7 = latex_table7.replace(\"F Statistic\", \"% F Statistic\")\n",
    "latex_table7 = latex_table7.replace(\"\\\\textit{\",\"% \\\\textit{\")\n",
    "latex_table7 = latex_table7.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
    "latex_table7 = latex_table7.replace(\"\\\\end{table}\", \"\")\n",
    "\n",
    "print(latex_table7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
